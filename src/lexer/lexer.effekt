module lexer

import process

import context/context

def show(t: PreprocessorKeywordType): String = t match {
    case Module() => "module"
    case Include() => "include"
}

def show(t: KeywordType): String = t match {
    case Preprocessor(pWord) => "#"++show(pWord)
    case Wrapper() => "wrapper"
    case Layout() => "layout"
    case Fn() => "fn"
    case Pub() => "pub"
    case Return() => "return"
    case Let() => "let"
    case KeywordType::Void() => "void"
    case While() => "while"
    case Extern() => "extern"
    case If() => "if"
    case Else() => "else"
    case Mut() => "mut"
    case Defer() => "defer"
}

def show(t: SymbolType): String = t match {
    case LParen() => "("
    case RParen() => ")"
    case LBrace() => "{"
    case RBrace() => "}"
    case Comma() => ","
    case Colon() => ":"
    case Semicolon() => ";"
    case DotSym() => "."
    case DoubleColon() => "::"
    case Arrow() => "->"
    case Assign() => "="
    case Asterix() => "*"
    case LessSym() => "<"
    case GreatSym() => ">"
    case LessEq() => "<="
    case GreatEq() => ">="
    case Equals() => "=="
    case NotEq() => "!="
    case NotSym() => "!"
    case Minus() => "-"
    case Plus() => "+"
    case Divide() => "/"
    case AndSym() => "&&"
    case OrSym() => "||"
}

def show(t: DataType): String = t match {
    case BoolT(Some(true)) => "true"
    case BoolT(Some(false)) => "false"
    case BoolT(None()) => "bool"
    case StringT() => "string"
    case UIntT() => "u32"
    case IntT() => "i32"
    case FloatT() => "float"
    case UByteT() => "u8"
    case ByteT() => "i8"
    case PointerT(ptrType) => show(ptrType)++"*"
    case VoidT() => "void"
    case CustomT(name) => "custom type "++name
}

def show(t: TokenType): String = t match {
    case LiteralT(valueType) => show(valueType)++" literal"
    case IdentifierT() => "identifier"
    case Keyword(keywordType) => "keyword "++show(keywordType)
    case Symbol(symbolType) => show(symbolType)++" symbol"
    case TypeNameT(typeType) => "typename "++show(typeType)
    case EOF() => "EOF"
    case Assembly() => "assembly block"
}

def show(p: Position): String =
    show(p.line)++":"++show(p.col)

def show(t: Token): String =
    t.tokenType.show()++" ["++t.text++"]"

def infixEq(l: PreprocessorKeywordType, r: PreprocessorKeywordType): Bool = {
    (l, r) match {
        case (Module(), Module()) => true
        case (Include(), Include()) => true
        case _ => false
    }
}

def infixEq(l: KeywordType, r: KeywordType): Bool = {
    (l, r) match {
        case (Preprocessor(p1), Preprocessor(p2)) => infixEq(p1, p2)
        case (Wrapper(), Wrapper()) => true
        case (Layout(), Layout()) => true
        case (Fn(), Fn()) => true
        case (Pub(), Pub()) => true
        case (Return(), Return()) => true
        case (Let(), Let()) => true
        case (KeywordType::Void(), KeywordType::Void()) => true
        case (While(), While()) => true
        case (Extern(), Extern()) => true
        case (If(), If()) => true
        case (Else(), Else()) => true
        case (Mut(), Mut()) => true
        case _ => false
    }
}

def infixEq(l: SymbolType, r: SymbolType): Bool = {
    (l, r) match {
        case (LParen(), LParen()) => true
        case (RParen(), RParen()) => true
        case (LBrace(), LBrace()) => true
        case (RBrace(), RBrace()) => true
        case (Comma(), Comma()) => true
        case (Colon(), Colon()) => true
        case (Semicolon(), Semicolon()) => true
        case (DotSym(), DotSym()) => true
        case (DoubleColon(), DoubleColon()) => true
        case (Arrow(), Arrow()) => true
        case (Assign(), Assign()) => true
        case (Asterix(), Asterix()) => true
        case (LessSym(), LessSym()) => true
        case (GreatSym(), GreatSym()) => true
        case (LessEq(), LessEq()) => true
        case (GreatEq(), GreatEq()) => true
        case (Equals(), Equals()) => true
        case (NotEq(), NotEq()) => true
        case (NotSym(), NotSym()) => true
        case (Minus(), Minus()) => true
        case (Plus(), Plus()) => true
        case (Divide(), Divide()) => true
        case (AndSym(), AndSym()) => true
        case (OrSym(), OrSym()) => true
        case _ => false
    }
}

def infixEq(l: DataType, r: DataType): Bool = {
    (l, r) match {
        case (StringT(), StringT()) => true
        case (UIntT(), UIntT()) => true
        case (IntT(), IntT()) => true
        case (FloatT(), FloatT()) => true
        case (UByteT(), UByteT()) => true
        case (ByteT(), ByteT()) => true
        case (PointerT(p1), PointerT(p2)) => infixEq(p1, p2)
        case (VoidT(), VoidT()) => true
        case (CustomT(n1), CustomT(n2)) => n1 == n2
        case _ => false
    }
}

def infixEq(l: TokenType, r: TokenType): Bool = {
    (l, r) match {
        case (LiteralT(v1), LiteralT(v2)) => infixEq(v1, v2) 
        case (IdentifierT(), IdentifierT()) => true
        case (Keyword(k1), Keyword(k2)) => infixEq(k1, k2)
        case (Symbol(s1), Symbol(s2)) => infixEq(s1, s2)
        case (TypeNameT(t1), TypeNameT(t2)) => infixEq(t1, t2)
        case (EOF(), EOF()) => true
        case _ => false
    }
}

def infixEq(l: Position, r: Position): Bool = (l.line == r.line) && (l.col == r.col)

def infixEq(l: Token, r: Token): Bool = (l.tokenType == r.tokenType) // && (l.position == r.position)

def infixNeq(l: TokenType, r: TokenType): Bool = not(infixEq(l, r))

interface TokenListHandler {
    def remove_last(): Unit
    def add_token(t: Token): Token
}

interface Lexer {
    def current(): Token
    def peek(): Token
    def next(): Token
    def position(): Position
}

effect LexerError(msg: String, pos: Position): Nothing / CompilerContextRequest

def lexer_report[T]() { prog: () => T / {LexerError, InternalError} }: T / {CompilerContextRequest, InternalError} = {
    try { prog() }
    with LexerError { (msg, pos) => resume {
        val source: String = openRead(do getWorkingFile())
        val errLine: String = try {
                                val line = pos.line - 1
                                val lines = source.split("\n")
                                lines.get(line)
                            } with Exception[OutOfBounds] {
                                def raise(_, _) = "<unknown>"
                            }

        val spaces: String = " ".repeat(pos.col+7)
        val helpLine: String = getHelpLine()
    
        println("[LEXER ERROR] "++do getWorkingFile()++" at "++show(pos)++": "++msg)
        println("\n        "++errLine)
        println(spaces++"^")
        println(spaces++helpLine)
        //println("        "++msg)
        exit(1)
        <>
    }}
}

val symbols: List[Char] = [
    '(',')','{','}',',','.',':',';','*','-','>','=','<','!','-','+','/','&','|'
]

val builtinTypeNames: List[String] = [
    "u32", "i32", "u8", "i8", "float", "void", "bool"
]

def firstof[T](list: List[T]) { pred: T => Bool }: Option[T] = list match {
    case Cons(head, rest) => if (pred(head)) { Some(head) } else rest.firstof[T]{ pred }
    case Nil() => None()
}

effect skip_whitespace(): Bool

def skipWhiteSpace[T]{ prog: () => T / skip_whitespace } = {
    try { prog() }
    with skip_whitespace {
        resume(true)
    }
}

def countWhiteSpace[T]{ prog: () => T / skip_whitespace } = {
    try { prog() }
    with skip_whitespace {
        resume(false)
    }
}

def lexer[T](in: String) { prog: => T / {Lexer} } : T / {LexerError, TokenListHandler, CompilerContextRequest, InternalError} = {
    var line: Int = 1;
    var col: Int = 1;
    var index: Int = 0;
    var currentToken: Token = Token(EOF(), "", Position(0, 0, 0))

    def pos(textSize: Int): Position = Position(line, col-textSize, index)
    def is_eof(): Bool = index >= in.length
    def advance_line(): Unit = { line = line + 1; col = 1 }
    def change_index(amount: Int): Unit = { index = index + amount; col = col + amount; }

    try {
        def advance(): Char / skip_whitespace = {
            if (is_eof()) {
                in.charAt(index - 1)
            } else {
                change_index(1)
                val c: Char = in.charAt(index - 1)

                if (c.isWhitespace()) {
                    c match {
                        case '\n' => {
                            advance_line()
                            if (do skip_whitespace() && not(is_eof())) {
                                advance()
                            } else { c }
                        }
                        case _ => if (do skip_whitespace()) { 
                            advance()
                        } else { c }
                    }
                }
                else { c }
            }
        }

        def peek(): Char / skip_whitespace = {
            var currentPos = pos(0)
            val ch: Char = advance()
            line = currentPos.line
            col = currentPos.col
            index = currentPos.index
            ch
        }

        def handleSymbolToken(first: Char): Token / TokenListHandler =  {
            def handleSingle(c: Char): Token = c match {
                case '('  => do add_token(Token(Symbol(LParen()), c.show, pos(1)))
                case ')'  => do add_token(Token(Symbol(RParen()), c.show, pos(1)))
                case '{'  => do add_token(Token(Symbol(LBrace()), c.show, pos(1)))
                case '}'  => do add_token(Token(Symbol(RBrace()), c.show, pos(1)))
                case ','  => do add_token(Token(Symbol(Comma()), c.show, pos(1)))
                case ':'  => do add_token(Token(Symbol(Colon()), c.show, pos(1)))
                case ';'  => do add_token(Token(Symbol(Semicolon()), c.show, pos(1)))
                case '.'  => do add_token(Token(Symbol(DotSym()), c.show, pos(1)))
                case '='  => do add_token(Token(Symbol(Assign()), c.show, pos(1)))
                case '*'  => do add_token(Token(Symbol(Asterix()), c.show, pos(1)))
                case '<'  => do add_token(Token(Symbol(LessSym()), c.show, pos(1)))
                case '>'  => do add_token(Token(Symbol(GreatSym()), c.show, pos(1)))
                case '!'  => do add_token(Token(Symbol(NotSym()), c.show, pos(1)))
                case '-'  => do add_token(Token(Symbol(Minus()), c.show, pos(1)))
                case '+'  => do add_token(Token(Symbol(Plus()), c.show, pos(1)))
                case '/'  => do add_token(Token(Symbol(Divide()), c.show, pos(1)))
                case _ => do LexerError("Unexpected character '"++c.show++"'.", pos(1))
            }
            val curPos: Position = Position(line, col, index)
            if (is_eof()) { handleSingle(first) }
            else (first, countWhiteSpace { advance }) match {
                case (':', ':') => do add_token(Token(Symbol(DoubleColon()), "::", pos(2)))
                case ('-', '>') => do add_token(Token(Symbol(Arrow()), "->", pos(2)))
                case ('<', '=') => do add_token(Token(Symbol(LessEq()), "<=", pos(2)))
                case ('>', '=') => do add_token(Token(Symbol(GreatEq()), ">=", pos(2)))
                case ('=', '=') => do add_token(Token(Symbol(Equals()), "==", pos(2)))
                case ('!', '=') => do add_token(Token(Symbol(NotEq()), "!=", pos(2)))
                case ('&', '&') => do add_token(Token(Symbol(AndSym()), "&&", pos(2)))
                case ('|', '|') => do add_token(Token(Symbol(OrSym()), "||", pos(2)))
                case _ => {
                    line = curPos.line
                    col = curPos.col
                    index = curPos.index
                    handleSingle(first)
                }
            }
        }

        def handleLiteral(first: Char): Token / TokenListHandler = first match {
            case '"' => {
                val startIndex: Int = index
                while (not(is_eof()) && countWhiteSpace { peek } != '"') {
                    //if (countWhiteSpace{peek} == '\n') { }
                    val _ = countWhiteSpace{advance}
                }
                if (is_eof()) {
                    do LexerError("Unterminated String", pos(index - startIndex))
                }

                countWhiteSpace { advance }
                val literal: String = in.substring(startIndex, index - 1)
                do add_token(Token(LiteralT(StringT()), literal, pos(literal.length)))
            }
            case _ => {
                val startIndex: Int = index - 1
                var isFloat: Bool = false

                while (not(is_eof()) && ((countWhiteSpace{ peek }).isDigit() || countWhiteSpace{peek} == '.')) {
                    if (countWhiteSpace{peek} == '.') {
                        if (isFloat) do LexerError("Multiple '.' in number literal", pos(1))
                        val p = pos(0)
                        if (not(locally{countWhiteSpace{advance}; countWhiteSpace{peek}}.isDigit()))
                            do LexerError("Ending floating point literals with a '.' is not allowed.", p)
                        line = p.line
                        col = p.col
                        index = p.index
                        isFloat = true
                    }
                    val _ = countWhiteSpace{ advance }
                }

                val numberText: String = in.substring(startIndex, index)

                var suffix = ""
                while (not(is_eof()) && ((countWhiteSpace{peek}).isAlphabetic() || countWhiteSpace{peek}.isDigit)) {
                    suffix = suffix++(countWhiteSpace { advance }).toString()
                }

                suffix match {
                    case "u32" => do add_token(Token(LiteralT(UIntT()), numberText, pos(numberText.length)))
                    case "i32" => do add_token(Token(LiteralT(IntT()), numberText, pos(numberText.length)))
                    case "u8"  => do add_token(Token(LiteralT(UByteT()),  numberText, pos(numberText.length)))
                    case "i8"  => do add_token(Token(LiteralT(ByteT()),  numberText, pos(numberText.length)))
                    case "f" => do add_token(Token(LiteralT(FloatT()), numberText, pos(numberText.length)))
                    case "" =>
                        if (isFloat)
                            do add_token(Token(LiteralT(FloatT()), numberText, pos(numberText.length)))
                        else
                            do add_token(Token(LiteralT(IntT()), numberText, pos(numberText.length)))
                    case _ => do LexerError("Unknown numeric suffix.", pos(suffix.length))
                }
            }
        }
        
        def handleIdentifier(first: Char): Token / {TokenListHandler, InternalError} = {
            val startIndex: Int = index - 1

            while (not(is_eof()) && (countWhiteSpace{peek}.isAlphanumeric() || countWhiteSpace{peek} == '_')) {
                val _ = countWhiteSpace { advance }
            }

            var text: String = in.substring(startIndex, index)

            if (text == "_") {
                do LexerError("Standalone '_' is not a valid identifier", pos(1))
            }

            if (text.charAt(0).isUpper() || builtinTypeNames.contains(text){ (l, r) => l == r }) {
                var tt: DataType =
                    if (text.charAt(0).isUpper()) { CustomT(text) }
                    else text match { 
                        case "u32" => UIntT()
                        case "i32" => IntT()
                        case "u8" => UByteT()
                        case "i8" => ByteT()
                        case "float" => FloatT()
                        case "void" => VoidT()
                        case "bool" => BoolT(None())
                        case _ => do InternalError("This place should've been unreachable. Handle builtin type names corrently.", __FILE__, __LINE__)
                    }

                if (is_eof()) {
                    do add_token(Token(TypeNameT(tt), text, pos(text.length)))
                } else if (countWhiteSpace{peek} == '*') {
                    while (not(is_eof()) && skipWhiteSpace{peek} == '*') {
                        text = text++"*"
                        skipWhiteSpace { advance }
                        tt = PointerT(tt)
                    }

                    do add_token(Token(TypeNameT(tt), text, pos(text.length)))
                } else {
                    do add_token(Token(TypeNameT(tt), text, pos(text.length)))
                }
            } else text match {
                case "module" => do add_token(Token(Keyword(Preprocessor(Module())), text, pos(text.length)))
                case "include" => do add_token(Token(Keyword(Preprocessor(Include())), text, pos(text.length)))
                case "wrapper" => do add_token(Token(Keyword(Wrapper()), text, pos(text.length)))
                case "layout" => do add_token(Token(Keyword(Layout()), text, pos(text.length)))
                case "fn" => do add_token(Token(Keyword(Fn()), text, pos(text.length)))
                case "pub" => do add_token(Token(Keyword(Pub()), text, pos(text.length)))
                case "return" => do add_token(Token(Keyword(Return()), text, pos(text.length)))
                case "let" => do add_token(Token(Keyword(Let()), text, pos(text.length)))
                case "nullptr" => do add_token(Token(LiteralT(PointerT(VoidT())), text, pos(text.length)))
                case "true" => do add_token(Token(LiteralT(BoolT(Some(true))), text, pos(text.length)))
                case "false" => do add_token(Token(LiteralT(BoolT(Some(false))), text, pos(text.length)))
                case "if" => do add_token(Token(Keyword(If()), text, pos(text.length)))
                case "else" => do add_token(Token(Keyword(Else()), text, pos(text.length)))
                case "while" => do add_token(Token(Keyword(While()), text, pos(text.length)))
                case "extern" => do add_token(Token(Keyword(Extern()), text, pos(text.length)))
                case "mut" => do add_token(Token(Keyword(Mut()), text, pos(text.length)))
                case "defer" => do add_token(Token(Keyword(Defer()), text, pos(text.length)))
                case "asm" => {
                    if (skipWhiteSpace{advance} != '{')
                        do LexerError("Expected a block '{ ... }' after 'asm' directive.", pos(1))
                    val startIndex: Int = index
                    while (not(is_eof()) && countWhiteSpace{peek} != '}') {
                        val _ = countWhiteSpace{advance}
                    }
                    if (is_eof())
                        do LexerError("Unterminated assembly block.", pos(index - startIndex))
                    countWhiteSpace{advance}
                    val block: String = in.substring(startIndex, index - 1)
                    do add_token(Token(Assembly(), block, pos(block.length)))
                }
                case _ =>
                    do add_token(Token(IdentifierT(), text, pos(text.length)))
            }
        }
        
        def scanToken(): Token / TokenListHandler = {
            val ch: Char = skipWhiteSpace { advance }

            if (is_eof()) {
                do add_token(Token(EOF(), "", pos(0)))
                <>
            }

            if (ch == '/') {
                if (countWhiteSpace { peek } == '/') {
                    val curPos: Position = pos(1)
                    while (not(is_eof()) && countWhiteSpace{advance} != '\n') { }
                    scanToken()
                } else if ( countWhiteSpace { peek } == '*') {
                    val curPos: Position = pos(1)
                    var ident: Int = 1
                    while (not(is_eof()) && ident > 0) {
                        var ch: Char = skipWhiteSpace { advance }
                        if (ch == '/' && countWhiteSpace { peek } == '*')
                            ident = ident + 1
                        else if (ch == '*' && countWhiteSpace { peek } == '/')
                            ident = ident - 1
                    }
                    countWhiteSpace { advance }
                    if (is_eof()) { do LexerError("Unterminated comment block.", curPos) }
                    scanToken()
                } else {
                    currentToken = handleSymbolToken(ch)
                    currentToken
                }
            } else if (symbols.contains(ch){ (c1, c2) => c1 == c2 }) {
                currentToken = handleSymbolToken(ch)
                currentToken
            } else if (ch == '"' || ch.isDigit()) {
                currentToken = handleLiteral(ch)
                currentToken
            } else if (ch == '_' || ch.isAlphabetic()) {
                currentToken = handleIdentifier(ch)
                currentToken
            } else {
                do LexerError("Unexpected character '"++ch.toString()++"'", pos(1))
                <>
            }
        }

        def peekToken(): Token / TokenListHandler = {
            val indexToGoBack: Int = index
            val colToGoBack: Int = col
            val lineToGoBack: Int = line
            val token: Token = scanToken()
            index = indexToGoBack
            col = colToGoBack
            line = lineToGoBack

            do remove_last()
            token
        }

        try { prog() }
        with Lexer {
            def peek() = resume(peekToken())
            def next() = resume(scanToken())
            def current() = resume(currentToken)
            def position() = resume(pos(0))
        }
    } with Exception[OutOfBounds] {
        def raise(_, msg: String) = do LexerError(msg, pos(0))
    }
}

def handleTokenList() { prog: () => Unit / TokenListHandler }: List[Token] = {
    var tokenList: List[Token] = []

    try {
        prog()
        tokenList.reverse()
    } with TokenListHandler { 
        def add_token(t: Token) = {
            tokenList = Cons(t, tokenList)

            if (t.tokenType == EOF()) {
                tokenList.reverse()
            } else {
                resume(t)
            }
        }
        def remove_last() = tokenList match {
            case Cons(_, tail) => {
                tokenList = tail
                resume(())
            }
            case Nil() => resume(())
        }
    }
}

def lex(): List[Token] / {CompilerContextRequest, InternalError} = {
    with lexer_report;

    val source: String = openRead(do getWorkingFile())

    handleTokenList {
        lexer(source) {
            while ((do next()).tokenType != EOF()) {  }
        }
    }
}
